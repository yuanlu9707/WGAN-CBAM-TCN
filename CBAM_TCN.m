numObservations = numel(XTrain);numClasses = numel(classes);numFeatures = size(s.XTrain{1},1);numFilters = 512;filterSize = 7;dropoutFactor = 0.005;numBlocks = 4;layer = [sequenceInputLayer(numFeatures,Normalization="rescale-symmetric",Name="input")];lgraph = layerGraph(layer);outputName = "input";% outputName = layer.Name;for i = 1:numBlocks    dilationFactor = 2^(i-1);    name_cov1="conv1_"+i;    name_add="add_"+i;        layers = [        convolution1dLayer(filterSize,numFilters,DilationFactor=dilationFactor,Padding="causal",Name="conv1_"+i)        layerNormalizationLayer        reluLayer        spatialDropoutLayer(dropoutFactor)        convolution1dLayer(filterSize,numFilters,DilationFactor=dilationFactor,Padding="causal")        layerNormalizationLayer        reluLayer        spatialDropoutLayer(dropoutFactor)        convolution1dLayer(filterSize,numFilters,DilationFactor=dilationFactor,Padding="causal")        layerNormalizationLayer        reluLayer        spatialDropoutLayer(dropoutFactor,Name="cbam_input1"+i)        ];    lgraph = addLayers(lgraph,layers);        layers = [        globalAveragePooling1dLayer(Name="avg1_1"+i)        fullyConnectedLayer(numFilters,Name="fc1_1"+i)        fullyConnectedLayer(numFilters,Name="fc1_2"+i)];    lgraph = addLayers(lgraph,layers);        layers = [        globalMaxPooling1dLayer(Name="max1_1"+i)        fullyConnectedLayer(numFilters,Name="fc2_1"+i)        fullyConnectedLayer(numFilters,Name="fc2_2"+i)        additionLayer(2,Name="adds"+i)        sigmoidLayer(Name="sigmoid_1"+i)        functionLayer(@(X) dlarray(X,"CBT"),Formattable=true,Name="m1"+i)        ];    lgraph = addLayers(lgraph,layers);                layers = [        multiplicationLayer(2,Name="multiplication_1"+i)        ];    lgraph = addLayers(lgraph,layers);        layers = [        averagePooling1dLayer(1,Name="avg2_1"+i)        concatenationLayer(1,2,Name="concatenation"+i)        convolution1dLayer(filterSize,1,Name="1dconv"+i,Padding="same")        sigmoidLayer(Name="sigmoid_2"+i)        functionLayer(@(X) dlarray(X,"CBT"),Formattable=true,Name="m_2"+i)        multiplicationLayer(2,Name="multiplication_2"+i)        additionLayer(2,Name="add_"+i)        ];    lgraph = addLayers(lgraph,layers);        layers = [        maxPooling1dLayer(1,Name="max2_1"+i)        ];    lgraph = addLayers(lgraph,layers);% Add and connect layers.	lgraph = connectLayers(lgraph,"cbam_input1"+i,"avg1_1"+i);    lgraph = connectLayers(lgraph,"cbam_input1"+i,"max1_1"+i);    lgraph = connectLayers(lgraph,"fc1_2"+i,"adds"+ i +"/in2");    lgraph = connectLayers(lgraph,"m1"+i,"multiplication_1"+ i +"/in1");    lgraph = connectLayers(lgraph,"cbam_input1"+i,"multiplication_1"+ i +"/in2");    lgraph = connectLayers(lgraph,"multiplication_1"+i,"avg2_1"+i);    lgraph = connectLayers(lgraph,"multiplication_1"+i,"max2_1"+i);    lgraph = connectLayers(lgraph,"max2_1"+i,"concatenation"+i +"/in2");    lgraph = connectLayers(lgraph,"multiplication_1"+i,"multiplication_2"+ i +"/in2");    lgraph = connectLayers(lgraph,outputName,"conv1_"+i);%     Skip connection.    if i == 1        % Include convolution in first skip connection.        layer = convolution1dLayer(1,numFilters,Name="convSkip");        lgraph = addLayers(lgraph,layer);        lgraph = connectLayers(lgraph,outputName,"convSkip");        lgraph = connectLayers(lgraph,"convSkip","add_" + i + "/in2");    else        lgraph = connectLayers(lgraph,outputName,"add_" + i + "/in2");    end        % Update layer output name.    outputName = "add_" + i;endlayers = [    fullyConnectedLayer(20,Name="fc")    fullyConnectedLayer(numClasses,Name="fc")    softmaxLayer    classificationLayer];lgraph = addLayers(lgraph,layers);lgraph = connectLayers(lgraph,outputName,"fc");% figure% plot(lgraph)% title("CBAM_TCN")% % xlim([0.95 4.23])% ylim([1.1 26.2])options = trainingOptions("adam", ...    ExecutionEnvironment='gpu', ...    MaxEpochs=30, ...    ValidationData={XTest_va,YTest_va}, ...    miniBatchSize=32, ...    InitialLearnRate=0.001, ...    LearnRateDropPeriod=20, ...     Plots="training-progress", ...    Verbose=0);net = trainNetwork(XTrain,YTrain,lgraph,options);